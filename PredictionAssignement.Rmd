---
title: "Practical Machine learning Peer assessment"
output: html_document
---

## Introduction
> **Background**
> Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

> **Data** 
> The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

> The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

> The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har


## Executive Summary
> The goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with.

> We have a dataset with to many columns and we need make a class prediction.
Firstly we prepare the dataset removing all columns with NA's or with empty values. We also remove the columns that clearly aren't predictor variables.

> Secondly We split training data into 60% training and 40% validating dataset.
After that, we try some transformations into training, like Preprocess by centering and scaling but we compared results and noticed that it´s no necessary.

> We  decide implement a random forests model, when we evaluate the model we get an Accuracy of 99% over Validation dataset.
We use the model to predict on the testing data set. 

> Finally we submit our predictions to the Assignment that return us that all of them are correct.


## Loading Libraries
```{r}
library(lattice); library(ggplot2);library(caret)
library(rpart);library(randomForest)
library(ElemStatLearn)
library(data.table)
```

### Reproducibility
```{r}
set.seed(321)
```

## Loading datasets
```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl))
testing <- read.csv(url(testUrl))                   

```

## Exploring and Preparing data
Our data has 160 variables, many of them with NA or empty values, given the complexity of the data, we decide to use the simplest approach, to remove all variables NA's or with empty values. 
We also remove the columns that clearly aren't predictor variables.
```{r}
# Function to remove features with any missing data
RemoveMissing <- function(d) {
  noMiss <- !sapply(d, function(x) any(is.na(x)))
  d <- d[, noMiss]
  
  noMiss <- !sapply(d, function(x) any(x==""))
  d <- d[, noMiss]
  return(d)
}  

# Applying to data
trainD<- RemoveMissing(training)
testD<- RemoveMissing(testing)

# Cleaning columns that clearly aren't predictor variables
col.rm <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", 
            "cvtd_timestamp", "new_window", "num_window")

d.rm <- which(colnames(trainD) %in% col.rm) 
trainD <- trainD[, -d.rm] 

d.rm <- which(colnames(testD) %in% col.rm) 
testD <- testD[, -d.rm] 

# convert classe into a factor
trainD$classe <- as.factor(trainD$classe)

# Remove the problem ID in testD
testD <- testD[,-ncol(testD)]

```

## Data Spliting
We split training data in training and validating data sets.
```{r}
inTrain = createDataPartition(trainD$classe, p=0.60, list=FALSE)
trainingD = trainD[inTrain,]
validatingD = trainD[-inTrain,]
```

## Preprocess training by centering and scaling.
```{r}
preObj <- preProcess(trainingD[, -ncol(trainingD)], method=c("center","scale"))
preObj

preClass<-predict(preObj,trainingD[, -ncol(trainingD)])
DTrainClass <- data.table(trainingD$classe, preClass)
names(DTrainClass)
```

Centering and scaling in validating dataset.
```{r}
preObjV <- preProcess(validatingD[, -ncol(validatingD)], method=c("center","scale"))

preClassV<-predict(preObj,validatingD[, -ncol(validatingD)])
DValClass <- data.table(validatingD$classe, preClassV)
```

Our output classe now is called "v1"
```{r}
#(trainingMod <- train(V1 ~ ., data=DTrainClass, method="rf"))
# Too much computation and it's not necessary
```

## Modeling
```{r}
# We use Random Forest with the training data set.
trainingmodel <- randomForest(classe ~ .,data=trainingD)
trainingmodel

# The importance of variables
varImp(trainingmodel)

# Evaluating the model
m <- predict(trainingmodel,newdata=validatingD[,-ncol(validatingD)])
confusionMatrix(m,validatingD$classe)
```
** We get an Accuracy of 99% over Validation dataset.**

## Predicting on the testing data
```{r}
predictions <- predict(trainingmodel,newdata=testD)
predictions
```

## Submission
Write submission files to `predictionAssignment_files/answers
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictions)

```